{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benjamin Wilke\n",
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Run one of the part-of-speech (POS) taggers available in Python. \n",
    "\n",
    "Find the longest sentence you can, longer than 10 words, that the POS tagger tags correctly. Show the input and output.\n",
    "\n",
    "Find the shortest sentence you can, shorter than 10 words, that the POS tagger fails to tag 100 percent correctly. Show the input and output. Explain your conjecture as to why the tagger might have been less than perfect with this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('received', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('votes', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('two', 'CD'),\n",
       " ('states', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('is', 'VBZ'),\n",
       " ('considered', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('favorite', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Nevada', 'NNP')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"He received the most votes in the first two states and is considered a favorite in Nevada\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Run a different POS tagger in Python. Process the same two sentences from question 1.\n",
    "\n",
    "Does it produce the same or different output?\n",
    "\n",
    "Explain any differences as best you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In a news article from this week’s news, find a random sentence of at least 10 words.\n",
    "\n",
    "Looking at the Penn tag set, manually POS tag the sentence yourself.\n",
    "\n",
    "Now run the same sentences through both taggers that you implemented for questions 1 and 2. Did either of the taggers produce the same results as you had created manually?\n",
    "\n",
    "Explain any differences between the two taggers and your manual tagging as much as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Prepare the train.txt Dataset for insertion into a RNN.\n",
    "\n",
    "The data is not in proper format.  Each line lists a word and 4 tags.  Your task is to form the words into sequences and form target sequences as well.\n",
    "\n",
    "Your target is the last (fourth) tag.\n",
    "\n",
    "Sentences are listed vertically.  A blank line indicates a new sentence.  \n",
    "\n",
    "Form the sentences into sequences\n",
    "\n",
    "Form the fourth tag into a sequence of targets.\n",
    "\n",
    "Encode the data to integers.  You will need two dictionaries:\n",
    "\n",
    "Word to integer\n",
    "\n",
    "Target Category to integer\n",
    "\n",
    "You should turn in the functions and code you use to perform this task—I will use them on a hidden dataset to test your success at encoding.  Padding is required—this data should be ready to put into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read().splitlines()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc = load_doc(\"train.txt\")                                           #<-- load raw doc\n",
    "blank_lines = [idx for idx, each in enumerate(raw_doc) if each == '']     #<-- get index of each blank line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses raw doc to return raw words and targets\n",
    "def getSentencesTargets(doc, blank_lines):\n",
    "    sentences = list()\n",
    "    targets = list()\n",
    "    \n",
    "    start_index = 0                                        #<-- start at index 0\n",
    "    end_index = blank_lines[0]                             #<-- set first end_index\n",
    "\n",
    "    for current_blank in range(len(blank_lines)):\n",
    "        sent_words = [word.split()[0].lower() for word in raw_doc[start_index:end_index]] #<-- isolate each word in the raw_doc range, lower()\n",
    "        sentences.append(sent_words)                                                      #<-- append sentence\n",
    "        target_classes = [word.split()[-1] for word in raw_doc[start_index:end_index]]    #<-- isolate each target in the raw_doc\n",
    "        targets.append(target_classes)                                                    #<-- append target\n",
    "        start_index = end_index + 1                        #<-- set next start_index, to last end_index + 1 (next line)\n",
    "        if current_blank < len(blank_lines) - 1:           #<-- if we're not at the end of the list (protect from out of range)\n",
    "            end_index = blank_lines[current_blank + 1]     #<-- then the new end_index is equal to next new end\n",
    "            \n",
    "    return sentences, targets            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns true if any character in a string is a number \n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans raw words and targets to remove non-words while also removing corresponding non-word targets\n",
    "def cleanSentencesTarget(sentences, targets):\n",
    "    remove_list = ['.',',','$','(',')','\\'s','\\\"',':','-','/','--','\\'',';']      #<-- specific characters we want to remove\n",
    "    \n",
    "    clean_sentences = list()\n",
    "    clean_targets = list()\n",
    "    \n",
    "    for sentence in range(len(sentences)):                      #<-- for each sentence\n",
    "        current_sentence = list()\n",
    "        current_target = list()                  \n",
    "        for idx, word in enumerate(sentences[sentence]):        #<-- examine each word\n",
    "            if word not in remove_list and not hasNumbers(word):#<-- if not in remove list or contains number\n",
    "                current_sentence.append(word)                   #<-- add word to current_sentence\n",
    "                current_target.append(targets[sentence][idx])   #<-- add corresponding target to current_target\n",
    "        if len(current_sentence) > 0:                           #<-- only add the current sentence if it's not null\n",
    "            clean_sentences.append(current_sentence)            #<-- append cleaned sentence \n",
    "            clean_targets.append(current_target)                #<-- append corresponding targets\n",
    "    \n",
    "    return clean_sentences, clean_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse raw sentence and target lists\n",
    "sentences, targets = getSentencesTargets(raw_doc, blank_lines)\n",
    "\n",
    "# clean sentence and target lists\n",
    "clean_sentences, clean_targets = cleanSentencesTarget(sentences, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(sentences):\n",
    "    word_counts = {}\n",
    "    for sent in sentences:\n",
    "        for w in sent:\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of all words and corresponding occurance counts\n",
    "word_counts = getWordCounts(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our word to index & index to word mapping \n",
    "def createWord2IndexIndex2Word(word_counts, word_count_threshold):\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    idxtoword = dict(enumerate(vocab, 3))\n",
    "    idxtoword[0] = '<PAD>'\n",
    "    idxtoword[1] = '<UNK>'\n",
    "    idxtoword[2] = '<START>'\n",
    "    wordtoix = dict([(value, key) for (key, value) in idxtoword.items()])\n",
    "    return wordtoix, idxtoword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our target (Y) category mapping\n",
    "def createTarget2CategoryIndex(targets):\n",
    "    unique_targets = list()\n",
    "    for target in targets:                                                      #<-- get a list of all eligible target values\n",
    "        for each in target:\n",
    "            unique_targets.append(each)\n",
    "    targettoix = {k:idx + 1 for idx, k in enumerate(list(set(unique_targets)))} #<-- saves room for <PAD> at 0\n",
    "    targettoix[\"<PAD>\"] = 0                                                     #<-- creates <PAD> at 0\n",
    "    return targettoix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our word to index & index to word mapping - only include words that occur more than 5 times\n",
    "wordToIndex, indexToWord = createWord2IndexIndex2Word(word_counts, 5)\n",
    "\n",
    "# create our target (Y) category mapping\n",
    "targetToIndex = createTarget2CategoryIndex(clean_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-MISC': 1,\n",
       " 'I-LOC': 2,\n",
       " 'B-MISC': 3,\n",
       " 'B-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'O': 6,\n",
       " 'I-PER': 7,\n",
       " 'B-ORG': 8,\n",
       " 'I-ORG': 9,\n",
       " '<PAD>': 0}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetToIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns sequence of index from word sentence input, if word doesn't exist sets to 1\n",
    "def sequenceFromSentenceList(sentence):\n",
    "    return [wordToIndex[word] if word in wordToIndex.keys() else 1 for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns sequence of words from index sentence input\n",
    "def sentenceFromSequenceList(sequence):\n",
    "    return [indexToWord[idx] for idx in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return sequence of target index from target sequence\n",
    "def sequenceFromTargetSequence(targetsequence):\n",
    "    return [targetToIndex[target] for target in targetsequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode our X and Y\n",
    "X_data = [sequenceFromSentenceList(sent) for sent in clean_sentences]\n",
    "Y_data = [sequenceFromTargetSequence(target) for target in clean_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest sequence length is: 83\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets look at sequence length\n",
    "seq_length = [len(sent) for sent in clean_sentences]\n",
    "print(\"The largest sequence length is: {}\".format(max(seq_length)))\n",
    "plt.hist(seq_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 35\n",
    "# now let's pad out X_data and Y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordCountDistribution(word_counts):\n",
    "    counts = [counts for words, counts in word_counts.items()]\n",
    "    plt.hist(counts, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPTF2",
   "language": "python",
   "name": "nlptf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
