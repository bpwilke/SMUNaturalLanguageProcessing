{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benjamin Wilke\n",
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pickle import dump\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Run one of the part-of-speech (POS) taggers available in Python. \n",
    "\n",
    "Find the longest sentence you can, longer than 10 words, that the POS tagger tags correctly. Show the input and output.\n",
    "\n",
    "Find the shortest sentence you can, shorter than 10 words, that the POS tagger fails to tag 100 percent correctly. Show the input and output. Explain your conjecture as to why the tagger might have been less than perfect with this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(word_tokenize(\"He received the most votes in the first two states and is considered a favorite in Nevada\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Run a different POS tagger in Python. Process the same two sentences from question 1.\n",
    "\n",
    "Does it produce the same or different output?\n",
    "\n",
    "Explain any differences as best you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In a news article from this week’s news, find a random sentence of at least 10 words.\n",
    "\n",
    "Looking at the Penn tag set, manually POS tag the sentence yourself.\n",
    "\n",
    "Now run the same sentences through both taggers that you implemented for questions 1 and 2. Did either of the taggers produce the same results as you had created manually?\n",
    "\n",
    "Explain any differences between the two taggers and your manual tagging as much as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Prepare the train.txt Dataset for insertion into a RNN.\n",
    "\n",
    "The data is not in proper format.  Each line lists a word and 4 tags.  Your task is to form the words into sequences and form target sequences as well.\n",
    "\n",
    "Your target is the last (fourth) tag.\n",
    "\n",
    "Sentences are listed vertically.  A blank line indicates a new sentence.  \n",
    "\n",
    "Form the sentences into sequences\n",
    "\n",
    "Form the fourth tag into a sequence of targets.\n",
    "\n",
    "Encode the data to integers.  You will need two dictionaries:\n",
    "\n",
    "Word to integer\n",
    "\n",
    "Target Category to integer\n",
    "\n",
    "You should turn in the functions and code you use to perform this task—I will use them on a hidden dataset to test your success at encoding.  Padding is required—this data should be ready to put into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read().splitlines()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc = load_doc(\"train.txt\")                                           #<-- load raw doc\n",
    "blank_lines = [idx for idx, each in enumerate(raw_doc) if each == '']     #<-- get index of each blank line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses raw doc to return raw words and targets\n",
    "def getSentencesTargets(doc, blank_lines):\n",
    "    sentences = list()\n",
    "    targets = list()\n",
    "    \n",
    "    start_index = 0                                        #<-- start at index 0\n",
    "    end_index = blank_lines[0]                             #<-- set first end_index\n",
    "\n",
    "    for current_blank in range(len(blank_lines)):\n",
    "        sent_words = [word.split()[0].lower() for word in raw_doc[start_index:end_index]] #<-- isolate each word in the raw_doc range, lower()\n",
    "        sentences.append(sent_words)                                                      #<-- append sentence\n",
    "        target_classes = [word.split()[-1] for word in raw_doc[start_index:end_index]]    #<-- isolate each target in the raw_doc\n",
    "        targets.append(target_classes)                                                    #<-- append target\n",
    "        start_index = end_index + 1                        #<-- set next start_index, to last end_index + 1 (next line)\n",
    "        if current_blank < len(blank_lines) - 1:           #<-- if we're not at the end of the list (protect from out of range)\n",
    "            end_index = blank_lines[current_blank + 1]     #<-- then the new end_index is equal to next new end\n",
    "            \n",
    "    return sentences, targets            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns true if any character in a string is a number \n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans raw words and targets to remove non-words while also removing corresponding non-word targets\n",
    "def cleanSentencesTarget(sentences, targets):\n",
    "    remove_list = ['.',',','$','(',')','\\'s','\\\"',':','-','/','--','\\'',';']      #<-- specific characters we want to remove\n",
    "    \n",
    "    clean_sentences = list()\n",
    "    clean_targets = list()\n",
    "    \n",
    "    for sentence in range(len(sentences)):                      #<-- for each sentence\n",
    "        current_sentence = list()\n",
    "        current_target = list()                  \n",
    "        for idx, word in enumerate(sentences[sentence]):        #<-- examine each word\n",
    "            if word not in remove_list and not hasNumbers(word):#<-- if not in remove list or contains number\n",
    "                current_sentence.append(word)                   #<-- add word to current_sentence\n",
    "                current_target.append(targets[sentence][idx])   #<-- add corresponding target to current_target\n",
    "        if len(current_sentence) > 0:                           #<-- only add the current sentence if it's not null\n",
    "            clean_sentences.append(current_sentence)            #<-- append cleaned sentence \n",
    "            clean_targets.append(current_target)                #<-- append corresponding targets\n",
    "    \n",
    "    return clean_sentences, clean_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse raw sentence and target lists\n",
    "sentences, targets = getSentencesTargets(raw_doc, blank_lines)\n",
    "\n",
    "# clean sentence and target lists\n",
    "clean_sentences, clean_targets = cleanSentencesTarget(sentences, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(sentences):\n",
    "    word_counts = {}\n",
    "    for sent in sentences:\n",
    "        for w in sent:\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of all words and corresponding occurance counts\n",
    "word_counts = getWordCounts(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our word to index & index to word mapping \n",
    "def createWord2IndexIndex2Word(word_counts, word_count_threshold):\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    idxtoword = dict(enumerate(vocab, 3))\n",
    "    idxtoword[0] = '<PAD>'\n",
    "    idxtoword[1] = '<UNK>'\n",
    "    idxtoword[2] = '<START>'\n",
    "    wordtoix = dict([(value, key) for (key, value) in idxtoword.items()])\n",
    "    return wordtoix, idxtoword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our target (Y) category mapping\n",
    "def createTarget2CategoryIndex(targets):\n",
    "    unique_targets = list()\n",
    "    for target in targets:                                                      #<-- get a list of all eligible target values\n",
    "        for each in target:\n",
    "            unique_targets.append(each)\n",
    "    targettoix = {k:idx + 1 for idx, k in enumerate(list(set(unique_targets)))} #<-- saves room for <PAD> at 0\n",
    "    targettoix[\"<PAD>\"] = 0                                                     #<-- creates <PAD> at 0\n",
    "    return targettoix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our word to index & index to word mapping - include all words since most of our Named Entities don't occur very often\n",
    "wordToIndex, indexToWord = createWord2IndexIndex2Word(word_counts, 1)\n",
    "\n",
    "# create our target (Y) category mapping\n",
    "targetToIndex = createTarget2CategoryIndex(clean_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns sequence of index from word sentence input, if word doesn't exist sets to 1\n",
    "def sequenceFromSentenceList(sentence):\n",
    "    return [wordToIndex[word] if word in wordToIndex.keys() else 1 for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns sequence of words from index sentence input\n",
    "def sentenceFromSequenceList(sequence):\n",
    "    return [indexToWord[idx] for idx in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return sequence of target index from target sequence\n",
    "def sequenceFromTargetSequence(targetsequence):\n",
    "    return [targetToIndex[target] for target in targetsequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode our X and Y\n",
    "X_data = [sequenceFromSentenceList(sent) for sent in clean_sentences]\n",
    "y_data = [sequenceFromTargetSequence(target) for target in clean_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest sequence length is: 75\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets look at sequence length\n",
    "seq_length = [len(sent) for sent in clean_sentences]\n",
    "print(\"The largest sequence length is: {}\".format(max(seq_length)))\n",
    "plt.hist(seq_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 35\n",
    "# now let's pad out X_data and Y_data\n",
    "X = sequence.pad_sequences(X_data, maxlen=max_seq_length)\n",
    "y = sequence.pad_sequences(y_data, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 100\n",
    "vocabulary_size = len(wordToIndex)\n",
    "n_tags = len(targetToIndex)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_vector_length, input_length=max_seq_length, mask_zero=True))\n",
    "model.add(LSTM(units=500, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 35, 100)           1715300   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 35, 500)           1202000   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 35, 10)            5010      \n",
      "=================================================================\n",
      "Total params: 2,922,310\n",
      "Trainable params: 2,922,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = X[3000:], X[:3000]\n",
    "y_train, y_val = y[3000:], y[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11724 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "11724/11724 [==============================] - 480s 41ms/sample - loss: 0.1628 - accuracy: 0.8457 - val_loss: 0.1089 - val_accuracy: 0.8925\n",
      "Epoch 2/3\n",
      "11724/11724 [==============================] - 501s 43ms/sample - loss: 0.0539 - accuracy: 0.9479 - val_loss: 0.0821 - val_accuracy: 0.9239\n",
      "Epoch 3/3\n",
      "11724/11724 [==============================] - 465s 40ms/sample - loss: 0.0304 - accuracy: 0.9693 - val_loss: 0.0790 - val_accuracy: 0.9270\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=8, epochs=3, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model so you don't have to train it again in case this notebook bombs\n",
    "dump(model, open('FirstTimeDistributedModel.h5', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what = model.predict_classes(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to EXCLUDE padding from confusion matrix\n",
    "count = 0\n",
    "total = 0\n",
    "pred = []\n",
    "actual = []\n",
    "for prediction in range(what.shape[0]):\n",
    "    for i in range(50):\n",
    "        if y_val[prediction][i]==41:\n",
    "            pass\n",
    "        else:\n",
    "            if y_val[prediction][i]==what[prediction][i]:\n",
    "                count = count+1\n",
    "            total=total+1\n",
    "            pred.append(what[prediction][i])\n",
    "            actual.append(y_val[prediction][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    all_val = list(set(actual+pred))\n",
    "    tick_marks = np.arange(len(all_val))\n",
    "    plt.xticks(tick_marks,[target_map[i] for i in all_val], rotation=45)\n",
    "    plt.yticks(tick_marks,[target_map[i] for i in all_val])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual, pred)\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPTF2",
   "language": "python",
   "name": "nlptf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
