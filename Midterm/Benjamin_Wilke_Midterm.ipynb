{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benjamin Wilke\n",
    "## Natural Language Processing - Midterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Essay 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Select one career or industry that makes use of applied NLP.\n",
    "\n",
    "Explain generally how that field or career utilizes NLP.\n",
    "\n",
    "Explain at least some methods of NLP that are very likely to be used in the career or industry you selected.\n",
    "\n",
    "Give at least one specific example of a use case for NLP within the chosen field, and explain how the problem or situation is (or could be) improved by applying NLP.\n",
    "</i></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can speak about how NLP is used in my own industry – digital advertising. In the beginning of digital advertising most advertisements were served based on keywords manually appended to content. For example – seeing an advertisement for Kraft foods on a recipe site. Today most targeted digital advertisements are enabled by cookies and mobile device identifiers that capture behavioral information across the open web, social platforms, and mobile apps. The example here is seeing an advertisement for a sweatshirt you looked at on Banana Republic while browsing ESPN (the sweatshirt has nothing to do with sports). However, there has been recent scrutiny on the privacy of Internet users and Google announced the deprecation of the 3rd-party cookie in their Chrome browser to follow suit with Firefox and Safari. With these concerns has come a new method for targeting users based on content – contextual targeting. While not unlike simple keyword targeting in the golden age of digital advertising – these contextual targeting platforms are able to apply much more information in an automated fashion utilizing NLP.\n",
    "\n",
    "The use cases for contextual targeting are two-fold. First – it can be used to ensure that your brand’s advertisements are running in a brand safe environment. Utilizing simple keywords appended manually by content authors or simply analyzing the URL is not a good indicator of the actual content of an article. Most advertisers try and avoid running advertisements alongside content falling into what are known as the “dirty dozen” toxic content categories, which include: pornography, crime, obscenity, terrorism, hate speech, and others. The other use case is to target users alongside relevant content, including late breaking news and trending themes that may otherwise not be found in targetable taxonomies. \n",
    "Given the large total sequence length of most Internet articles it’s likely that this technology utilizes simple bag-of-words methods for content classification (versus a method like a recurrent neural network). This can be achieved through the utilization of TF/IDF in a supervised manner by comparing new content to known content. To enable the ability to quickly target late breaking news articles can be identified based on new words and word combinations, while also scoring the new content in terms of sentiment or the presence of non-brand safe content.\n",
    "\n",
    "I work for Oracle Data Cloud and we recently acquired Grapeshot, which is a contextual targeting platform that was cofounded by Dr. Martin Porter (who invented the Porter stemmer).\n",
    "\n",
    "https://www.oracle.com/a/ocom/docs/corporate/acquisitions/grapeshot-methodology.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Essay 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Choose one of the “trade-offs” in NLP that was covered in the asynchronous materials for this course.\n",
    "\n",
    "Explain the trade-off in general terms. Define the two choices.\n",
    "\n",
    "Explain the benefits and weaknesses of each side of the trade-off.  Include at least one benefit and one weakness of each.\n",
    "\n",
    "Describe a work-situation that would make one of the choices in the trade-off much better, in terms of practical outcomes for you and your stakeholders on a project.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not unique to NLP, one of the interesting “trade-offs” in NLP is transparent vs. opaque. These terms describe an algorithm and are sometimes substituted with “AI” (artificial intelligence) vs. “XAI” (explainable artificial intelligence). Often in machine learning there is tradeoff between algorithms that are verbose in how they are classifying an outcome (\"transparent\") vs. a “black box” where specific qualities related to an outcome aren’t known (\"opaque\"). It turns out that black box methods tend to be more powerful, so it’s not as easy as simply opting for a more transparent approach/algorithm.\n",
    "\n",
    "The benefit of a transparent algorithm is it often has readily interpretable results. For example – being able to understand specifically which features contribute to an outcome (and by how much). This is often helpful when a business case requires understanding about which aspects of the data should be further explored to optimize a result. Unfortunately, a weakness of transparent algorithms is that they may not provide the most predictive power as statistical subtleties and interactions between features are often excluded from the models (as they cannot easily be explained).\n",
    "\n",
    "Opaque algorithms often excel at pure predictive power and accuracy and are mostly dominated by the deep learning approaches that have taken the data science community by storm. Business cases where these algorithms are relevant concern improving the accuracy of predictions at the expense of interpretability. That is to say – the model may have fantastic predictive power, but drilling into one of the potentially millions of parameters in a deep neural network has no practical interpretation.\n",
    "\n",
    "An example of the application of a transparent algorithm is estimating home prices using regression. Perhaps the business stakeholders are willing to have less accurate forecasting if they can understand the top 3 contributors to the home price. This information can be used to inform marketing messaging or sales prospecting.\n",
    "\n",
    "Finally, an example of applying an opaque “black box” algorithm would be using a recurrent neural network to classify sentiment in movie reviews (like we’ve done in this class). The business stakeholders likely don’t care about which specific words or interactions between words harness the most predictive power, but rather they care about maximizing the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Label each block and step by input/sequence step.  Compute the dimensions of the weight for all steps.  All inputs must be labeled by dimension.  Include your original word ENCODING (notice not vector!) as input.  You may omit bias!</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 8, 80)             800       \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 8, 115)            67965     \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 95)                60420     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 96        \n",
      "=================================================================\n",
      "Total params: 129,281\n",
      "Trainable params: 129,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense\n",
    "\n",
    "max_sent_length = 8\n",
    "\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=max_sent_length)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_sent_length)\n",
    "\n",
    "embedding_vecor_length = 80\n",
    "\n",
    "vocab = {'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumped': 4, 'over': 5, 'fence': 6, 'under': 7, 'car' : 8, 'did': 9}\n",
    "\n",
    "top_words = len(vocab) #<-- had to get the length of vocabulary as input to determine the shape of embedding layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_sent_length))\n",
    "model.add(GRU(115, return_sequences=True))\n",
    "model.add(GRU(95))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](GRUNetworkDiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Write the initial vector form of the input sequence using only 1s and 0s</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<<PAD>>': 0,\n",
       " 'the': 1,\n",
       " 'quick': 2,\n",
       " 'brown': 3,\n",
       " 'fox': 4,\n",
       " 'jumped': 5,\n",
       " 'over': 6,\n",
       " 'fence': 7,\n",
       " 'under': 8,\n",
       " 'car': 9,\n",
       " 'did': 10}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'<<PAD>>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumped': 5, 'over': 6, 'fence': 7, 'under': 8, 'car' : 9, 'did': 10}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added PAD at index 0, which increases the vocabulary size to 11. This means the one-hot encoded sequence will need to be 8 x 11, which is max_sent_length x vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vec = np.array([[1,0,0,0,0,0,0,0,0,0,0],  # <<PAD>>\n",
    "                       [1,0,0,0,0,0,0,0,0,0,0],  # <<PAD>>\n",
    "          [0,1,0,0,0,0,0,0,0,0,0],  # the\n",
    "          [0,0,0,0,0,0,0,0,0,1,0],  # car\n",
    "          [0,0,0,0,0,1,0,0,0,0,0],  # jumped\n",
    "          [0,0,0,0,0,0,1,0,0,0,0],  # over \n",
    "          [0,1,0,0,0,0,0,0,0,0,0],  # the\n",
    "          [0,0,0,0,0,0,0,1,0,0,0],  # fence\n",
    "        ])\n",
    "\n",
    "onehot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 11)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Find the average Glove Word Vector of your input sequence - \"the car jumped over the fence\" (Spacy uses Glove vectors!)!</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.02246672e-01,  6.65950105e-02, -1.49527833e-01,  3.49901654e-02,\n",
       "        2.13985667e-01, -7.65089318e-02, -2.45101675e-01,  7.83283338e-02,\n",
       "        2.09699962e-02,  2.41605020e+00, -1.05761170e-01,  5.89891970e-02,\n",
       "        3.98833267e-02, -6.72252774e-02, -2.43400812e-01, -1.21773286e-02,\n",
       "       -7.25516751e-02,  1.03658164e+00, -2.32895855e-02, -1.55521646e-01,\n",
       "        2.29424983e-02, -8.47426429e-02,  2.09339336e-01,  1.32689821e-02,\n",
       "        1.60996635e-02, -1.53346330e-01, -8.59160051e-02, -1.78890824e-01,\n",
       "       -9.34920013e-02, -2.61886623e-02,  4.54970933e-02,  2.78918356e-01,\n",
       "       -1.12586327e-01,  2.80501485e-01,  1.23955004e-01, -1.41709670e-01,\n",
       "       -4.83516753e-02,  1.34987980e-02,  1.60396993e-02, -3.77281681e-02,\n",
       "        2.11798310e-01,  4.15756665e-02, -5.41546196e-02, -6.71449974e-02,\n",
       "        8.97116885e-02, -4.67299968e-02, -2.15548679e-01, -3.80335063e-01,\n",
       "        6.24183305e-02,  1.80273965e-01, -4.30923253e-02, -6.58916831e-02,\n",
       "       -1.00524008e-01,  1.68676674e-01, -1.23110332e-01,  3.10916658e-02,\n",
       "        4.90240008e-02, -6.93003386e-02, -5.66300005e-02, -6.21146746e-02,\n",
       "        1.06594689e-01,  1.92563340e-01, -2.24631652e-02,  2.88225979e-01,\n",
       "        6.67833313e-02,  3.77099998e-02, -3.47989984e-02, -1.93191484e-01,\n",
       "        1.17071338e-01,  1.61466673e-01, -1.18047833e-01, -1.62384976e-02,\n",
       "        3.14163327e-01,  2.70906657e-01, -4.48926575e-02, -4.22549956e-02,\n",
       "        1.23116501e-01, -2.96609342e-01, -1.39579833e-01,  8.14026669e-02,\n",
       "        1.10720165e-01,  3.54271650e-01,  1.22986631e-02, -3.63661647e-02,\n",
       "        1.21970713e-01,  1.98808834e-01,  7.96278417e-02,  2.64610112e-01,\n",
       "        1.20267332e-01,  1.64494321e-01, -6.51488379e-02, -3.08013353e-02,\n",
       "        6.57665059e-02, -1.18793339e-01, -1.21375002e-01,  3.01500154e-03,\n",
       "        4.15491648e-02,  1.44447833e-01,  3.20313334e-01, -2.33143326e-02,\n",
       "        1.13832675e-01,  5.34916669e-02, -2.11709991e-01, -1.60306171e-01,\n",
       "        3.01343352e-02, -6.81866646e-01,  1.71360001e-01,  3.60621698e-02,\n",
       "       -1.91083346e-02, -8.79466757e-02,  9.86851528e-02, -9.48226675e-02,\n",
       "        3.48386019e-01, -1.45902663e-01,  2.00064316e-01,  6.77258372e-02,\n",
       "       -1.20133413e-02,  6.13599978e-02,  1.19159997e-01,  3.63789983e-02,\n",
       "        2.38830000e-02, -1.61527157e-01, -1.02036051e-01, -3.16459656e-01,\n",
       "       -2.49000326e-01, -2.51293313e-02, -5.27970009e-02, -1.01913333e-01,\n",
       "       -4.52106707e-02,  4.09161337e-02, -2.05126658e-01,  1.94621012e-01,\n",
       "       -3.07096660e-01,  6.72188327e-02,  6.60986602e-02, -1.50133327e-01,\n",
       "        2.80357990e-02,  2.23969996e-01,  2.87990030e-02, -9.97816697e-02,\n",
       "       -1.43598330e+00, -5.37484922e-02,  1.56250343e-01,  9.46183223e-03,\n",
       "        1.09009333e-01, -3.36200255e-03, -4.48800027e-02, -3.68850045e-02,\n",
       "        2.72751659e-01,  1.41264990e-01, -2.70038337e-01, -7.61556700e-02,\n",
       "        1.36434823e-01, -8.98864940e-02, -8.02138373e-02, -2.03154027e-01,\n",
       "        1.20266654e-01,  3.84640008e-01,  2.84733325e-02,  6.68138266e-02,\n",
       "       -1.45843834e-01,  8.97001699e-02, -1.28053337e-01, -1.00520827e-01,\n",
       "       -4.73484956e-02, -1.14998333e-01,  6.95599988e-02, -3.95846665e-02,\n",
       "        1.30314782e-01,  1.50368169e-01,  1.25569835e-01,  5.13156652e-02,\n",
       "       -1.71585009e-01, -7.21875057e-02, -2.73713320e-01, -3.97153310e-02,\n",
       "       -5.06145023e-02,  2.36549005e-01,  2.91716699e-02, -1.71874329e-01,\n",
       "        1.51221335e-01, -1.54766232e-01, -2.89241672e-01, -1.96124002e-01,\n",
       "       -5.10866335e-03, -1.69838175e-01, -2.17521656e-02, -1.67769507e-01,\n",
       "        3.57716642e-02, -4.23661657e-02,  2.57701337e-01,  7.14846626e-02,\n",
       "       -7.01555312e-02,  1.91963330e-01, -1.32479936e-01,  1.03625663e-01,\n",
       "        1.56697720e-01, -2.53173355e-02, -2.86182493e-01,  7.02083623e-03,\n",
       "       -4.01316732e-02, -2.23066639e-02, -2.45521680e-01,  6.72115162e-02,\n",
       "        2.54228026e-01,  5.75384684e-02,  1.05767667e-01, -1.16228998e-01,\n",
       "        3.61099951e-02, -1.28049990e-02, -5.97448312e-02,  1.28031671e-02,\n",
       "        3.52631658e-02,  6.83058724e-02,  8.96621570e-02,  1.96179509e-01,\n",
       "       -2.67955840e-01,  2.30717853e-01, -7.25523308e-02, -1.13187671e-01,\n",
       "        2.49129981e-02, -1.05219662e-01, -1.22388333e-01, -1.36076659e-02,\n",
       "       -2.43960977e-01, -3.24010491e-01,  6.23329252e-04,  1.39413312e-01,\n",
       "       -2.04223339e-02,  7.26416633e-02, -9.96336341e-04,  7.41533339e-02,\n",
       "       -9.62996706e-02, -3.63566726e-02, -6.97279871e-02, -7.91983381e-02,\n",
       "        9.13333297e-02,  1.53121501e-01,  1.03963353e-02,  7.81561658e-02,\n",
       "       -8.12756717e-02, -3.50351669e-02,  4.68028337e-02,  1.41970322e-01,\n",
       "       -5.43585867e-02, -3.55706662e-01,  1.19163997e-01, -1.45466819e-01,\n",
       "       -8.59850049e-02, -4.72178273e-02, -1.90006986e-01, -6.41948357e-02,\n",
       "        1.46570489e-01,  4.02814709e-02,  5.15481681e-02,  3.15691680e-01,\n",
       "        1.67254999e-01,  1.07822001e-01,  1.07833361e-02, -1.48848668e-01,\n",
       "        2.58671850e-01,  5.72443306e-02,  8.61733258e-02, -3.16029310e-01,\n",
       "        2.91208327e-01, -2.62440503e-01, -7.26199970e-02, -1.19893990e-01,\n",
       "       -4.26665060e-02, -1.96394488e-01,  2.56954014e-01,  7.53486678e-02,\n",
       "        4.77932766e-03, -1.09933324e-01,  8.91481638e-02, -3.66953202e-02,\n",
       "        7.48030022e-02,  1.68176994e-01,  2.46623307e-02, -3.43943350e-02,\n",
       "       -7.04811588e-02,  3.51395011e-02, -1.66051671e-01,  1.31367818e-01,\n",
       "        6.32465035e-02, -4.31626625e-02, -4.79533756e-03,  1.37216842e-03,\n",
       "       -4.09579948e-02, -5.88233359e-02, -5.01784980e-01, -1.69103339e-01,\n",
       "        2.81081647e-01, -1.91766188e-01, -6.20938353e-02, -9.28726569e-02,\n",
       "        1.14023328e-01,  1.77116826e-01,  9.68199968e-02,  6.52955025e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_seq = (nlp(\"the\").vector + nlp(\"car\").vector + nlp(\"jumped\").vector + nlp(\"over\").vector + nlp(\"the\").vector + nlp(\"fence\").vector) / 6\n",
    "average_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Find the nearest word (in the above dictionary) to the average calculated in previous question</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.24667346477508545),\n",
       " ('quick', 0.532218724489212),\n",
       " ('brown', 0.716812789440155),\n",
       " ('fox', 0.689554899930954),\n",
       " ('jumped', 0.3494679927825928),\n",
       " ('over', 0.25146275758743286),\n",
       " ('fence', 0.3551366925239563),\n",
       " ('under', 0.508454829454422),\n",
       " ('car', 0.3474304676055908),\n",
       " ('did', 0.4202675223350525)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# The Cosine Similarity values for different documents, 1 (same direction), 0 (90 deg.), -1 (opposite directions).\n",
    "# Cosine Distance, 1 - Cosine Similarity, then can take on values from 0 to 2 \n",
    "\n",
    "vocab = {'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumped': 4, 'over': 5, 'fence': 6, 'under': 7, 'car' : 8, 'did': 9}\n",
    "\n",
    "list(zip(vocab, [distance.cosine(average_seq, nlp(word).vector) for word, idx in vocab.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly the average GloVe vector from our input sequence is closest to the word \"the\". This is mostly due to the fact that the word \"the\" is in the input sequence twice. Interestingly - the next most similar word is \"over\", which means that \"over\" must be more similar to \"the\" than to \"car\" or \"jumped\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45474982261657715"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(nlp(\"the\").vector, nlp(\"over\").vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6812600791454315"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(nlp(\"the\").vector, nlp(\"car\").vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429614961147308"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(nlp(\"the\").vector, nlp(\"jumped\").vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>What is the difference between the W(weight) matrix of the first GRU sequence at time/sequence 0 and at time/sequence 5.  How do you know this?</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no difference in the weight matrix itself as backpropagation has not yet occurred. Once the final GRU output is produced, gradients can be calculated from the loss and the weights will be updated before the next sequence is loaded.\n",
    "\n",
    "However, we can say that the current hidden state of the GRU reccurrent layer has been informed by each word (represesnted by word embedding) for time/sequence 0-4. The hidden state for time 0-4 is fed into the current GRU iteration with the word embedding for time/sequence 5 to produce the next hidden state to pass to time/sequence 6, until the total sequence length (in our example, 8) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>What is missing in the above code—something important is not determined and based on that, there are some minor adjustments or additions that need to be made\n",
    "    \n",
    "Make a logical determination of what that missing piece of info should be based on the info given here and what additions or adjustments are necessary </i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is referring to the vocabulary size and/or the addition of padding and unknown tokens/indices in the dictionary, which are some of the things I had to consider to answer the questions in this midterm.\n",
    "\n",
    "I first noticed this when top_words wasn't defined in the professor's code. This is required input to the embedding layer as the shape of the embedding weights is (vocabulary size x embedding length). \"top_words\" also implies that the code the professor was using was likely analyzing the most used words in the corpus to determine which words to keep in the dictionary. Words that aren't in the dictionary get mapped to an \"unknown\" token and this is generally done to speed up processing and/or reduce the size of the total vocabulary in memory. \n",
    "\n",
    "Assuming professor wanted us to maintain the sequence length of 8 for question 2 I had to introduce a padding token and index to use to extend the sequence length to 8. It's always a best practice to include the padding and unknown token/index from the beginning of dictionary creation as they are almost always needed for sequence learning natural language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPTF2",
   "language": "python",
   "name": "nlptf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
