{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benjamin Wilke\n",
    "# Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Downloaded from Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pg14640.txt\n",
    "McGuffey's First Eclectic Reader, Revised Edition by William Holmes McGuffey\n",
    "http://www.gutenberg.org/cache/epub/14640/pg14640.txt\n",
    "\n",
    "pg14880.txt\n",
    "McGuffey's Fourth Eclectic Reader by William Holmes McGuffey\n",
    "http://www.gutenberg.org/cache/epub/14880/pg14880.txt\n",
    "\n",
    "pg16751.txt\n",
    "McGuffey's Sixth Eclectic Reader by William Holmes McGuffey\n",
    "http://www.gutenberg.org/cache/epub/16751/pg16751.txt\n",
    "\n",
    "pg19721.txt\n",
    "The Literary World Seventh Reader by Browne, Metcalf, and Withers\n",
    "http://www.gutenberg.org/cache/epub/19721/pg19721.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader and Tokenizer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "def open_Gutenberg(url):\n",
    "    response = request.urlopen(url)\n",
    "    return response.read().decode('utf8')\n",
    "\n",
    "def loadAndPrepReader(file_contents):\n",
    "    file_contents = file_contents.split('\\n')   # split into lines\n",
    "    file_contents = [line.strip('\\r') for line in file_contents]  # remove carriage-returns\n",
    "    start_index = [i for i, s in enumerate(file_contents) if '*** START OF THIS PROJECT' in s] # get start index\n",
    "    end_index = [i for i, s in enumerate(file_contents) if '*** END OF THIS PROJECT' in s] # get end index\n",
    "    return file_contents[start_index[0]+1:end_index[0]-1] # returned sliced file_contents\n",
    "\n",
    "def tokenizer(file_contents):\n",
    "    tokenized_words = [word_tokenize(line) for line in file_contents] # tokenize our raw list of lines\n",
    "    unroll = [item for sublist in tokenized_words for item in sublist] # unroll into single list\n",
    "    return [w for w in unroll if w.isalnum()] # drop special characters, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcGuffey_one = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/14640/pg14640.txt\")))\n",
    "mcGuffey_four = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/14880/pg14880.txt\")))\n",
    "mcGuffey_six = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/16751/pg16751.txt\")))\n",
    "literaryWorld_seven = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/19721/pg19721.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Text Difficulty\" Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns percentage of unique words divided by total words, \"lexical diversity\"\n",
    "# I don't like that this lexical_diversity function doesn't account for case, but following the example in the\n",
    "# NLTK book\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def vocab_size(text):\n",
    "    return len(set(word.lower() for word in text)) # this does account for case\n",
    "\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lexical diversity of McGuffey's First Eclectic Reader: 0.18\n",
      "The lexical diversity of McGuffey's Fourth Eclectic Reader: 0.14\n",
      "The lexical diversity of McGuffey's Sixth Eclectic Reader: 0.12\n",
      "The lexical diversity of The Literary World Seventh Reader: 0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"The lexical diversity of McGuffey's First Eclectic Reader: {0:.2f}\".format(lexical_diversity(mcGuffey_one)))\n",
    "print(\"The lexical diversity of McGuffey's Fourth Eclectic Reader: {0:.2f}\".format(lexical_diversity(mcGuffey_four)))\n",
    "print(\"The lexical diversity of McGuffey's Sixth Eclectic Reader: {0:.2f}\".format(lexical_diversity(mcGuffey_six)))\n",
    "print(\"The lexical diversity of The Literary World Seventh Reader: {0:.2f}\".format(lexical_diversity(literaryWorld_seven)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem to align with my expectations. That is - for more basic readers the number of unique words is large(r) compared to the total number of words. This means that the unique words take up more of the total words, which could be a proxy for the level of the writing style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size of McGuffey's First Eclectic Reader: 1141\n",
      "The vocabulary size of McGuffey's Fourth Eclectic Reader: 7579\n",
      "The vocabulary size of McGuffey's Sixth Eclectic Reader: 14038\n",
      "The vocabulary size of The Literary World Seventh Reader: 10220\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary size of McGuffey's First Eclectic Reader: {0}\".format(vocab_size(mcGuffey_one)))\n",
    "print(\"The vocabulary size of McGuffey's Fourth Eclectic Reader: {0}\".format(vocab_size(mcGuffey_four)))\n",
    "print(\"The vocabulary size of McGuffey's Sixth Eclectic Reader: {0}\".format(vocab_size(mcGuffey_six)))\n",
    "print(\"The vocabulary size of The Literary World Seventh Reader: {0}\".format(vocab_size(literaryWorld_seven)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also seems to align with my expectations. The more advanced readers have a higher total vocabulary size. This is clear examining from first, fourth, and sixth. The seventh reader seems to drop back down, but may simply be related to the fact that it's a different series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vocabulary Size & Lexical Diversity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first issue is that lexical diversity on its own is a poor measure of \"lexical richness\" because usually texts containing a large number of tokens will have lower lexical diversity scores simply based on the fact that the writer must re-use function words (words that have litle lexical meaning). A second issue could be that a text contains a very large vocabulary, but is succint in it's writing, thus producing a high lexical diversity score (but could be quite advanced in terms of the vocabulary). To solve the first issue lexical diversity makes more sense when comparing texts that have similar total token size. The second issue can be analyzed by looking at both lexical diversity and the vocabulary size. Thus I would argue that both measures are required to appropriately measure text difficulty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
