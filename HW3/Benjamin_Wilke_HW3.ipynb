{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benjamin Wilke\n",
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your given name with your nickname (if you donâ€™t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "\n",
    "What is the edit distance between your nickname and your given name?\n",
    "\n",
    "What is the percentage string match between your nickname and your given name?\n",
    "\n",
    "Show your work for both calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nick = \"Ben\"\n",
    "full = \"Benjamin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Benjamin\" - \"Ben\" = \"jamin\", which is length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://towardsdatascience.com/calculating-string-similarity-in-python-276e18a7d33a\n",
    "import Levenshtein\n",
    "Levenshtein.distance(nick, full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage string match is (3 / 8) * 100 = 37.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words.\n",
    "\n",
    "Did you friend correctly guess the book on the first try?\n",
    "\n",
    "What did he or she guess?\n",
    "\n",
    "Explain why you think you friend either was or was not able to guess the book from hearing the list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first two sentences from Jurrasic Park\n",
    "jurassic_park = \"The late twentieth century has witnessed a scientific gold rush of astonishing proportions: the \\\n",
    "headlong and furious haste to commercialize genetic engineering. This enterprise has proceeded \\\n",
    "so rapidly - with so little outside commentary - that its dimensions and implications are hardly \\\n",
    "understood at all.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Source Text \n",
      "\n",
      "['The', 'late', 'twentieth', 'century', 'has', 'witnessed', 'a', 'scientific', 'gold', 'rush', 'of', 'astonishing', 'proportions', 'the', 'headlong', 'and', 'furious', 'haste', 'to', 'commercialize', 'genetic', 'engineering', 'This', 'enterprise', 'has', 'proceeded', 'so', 'rapidly', 'with', 'so', 'little', 'outside', 'commentary', 'that', 'its', 'dimensions', 'and', 'implications', 'are', 'hardly', 'understood', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = [w for w in word_tokenize(jurassic_park) if not w in string.punctuation]\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(\"Tokenized Source Text \\n\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with Stop Words Removed \n",
      "\n",
      "['The', 'late', 'twentieth', 'century', 'witnessed', 'scientific', 'gold', 'rush', 'astonishing', 'proportions', 'headlong', 'furious', 'haste', 'commercialize', 'genetic', 'engineering', 'This', 'enterprise', 'proceeded', 'rapidly', 'little', 'outside', 'commentary', 'dimensions', 'implications', 'hardly', 'understood']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Text with Stop Words Removed \\n\")\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My friend did actually guess the book correctly with a subtle hint that it was written by Michael Crichton. Although, Westworld may have also been a good candidate.\n",
    "\n",
    "I think he was able to guess based on the inclusion of several key words, which would leave one to believe these sentences are about genetic engineering and the repercussions of hasty scientific pursuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the stemmers available in Python.\n",
    "\n",
    "Run the same two sentences from question 2 above through the stemmer and show the results.\n",
    "\n",
    "How many of the outputted stems are valid morphological roots of the corresponding words?\n",
    "\n",
    "Express this answer as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'The'), ('late', 'late'), ('twentieth', 'twentieth'), ('centuri', 'century'), ('ha', 'has'), ('wit', 'witnessed'), ('a', 'a'), ('scientif', 'scientific'), ('gold', 'gold'), ('rush', 'rush'), ('of', 'of'), ('astonish', 'astonishing'), ('proport', 'proportions'), ('the', 'the'), ('headlong', 'headlong'), ('and', 'and'), ('furiou', 'furious'), ('hast', 'haste'), ('to', 'to'), ('commerci', 'commercialize'), ('genet', 'genetic'), ('engin', 'engineering'), ('thi', 'This'), ('enterpris', 'enterprise'), ('ha', 'has'), ('proceed', 'proceeded'), ('so', 'so'), ('rapidli', 'rapidly'), ('with', 'with'), ('so', 'so'), ('littl', 'little'), ('outsid', 'outside'), ('commentari', 'commentary'), ('that', 'that'), ('it', 'its'), ('dimens', 'dimensions'), ('and', 'and'), ('implic', 'implications'), ('are', 'are'), ('hardli', 'hardly'), ('understood', 'understood'), ('at', 'at'), ('all', 'all')]\n"
     ]
    }
   ],
   "source": [
    "stems = (list(zip([ps.stem(w) for w in word_tokens], word_tokens)))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 invalid output stems. \n",
      "\n",
      "('centuri', 'century')\n",
      "('ha', 'has')\n",
      "('wit', 'witnessed')\n",
      "('scientif', 'scientific')\n",
      "('astonish', 'astonishing')\n",
      "('proport', 'proportions')\n",
      "('furiou', 'furious')\n",
      "('hast', 'haste')\n",
      "('commerci', 'commercialize')\n",
      "('genet', 'genetic')\n",
      "('engin', 'engineering')\n",
      "('thi', 'This')\n",
      "('enterpris', 'enterprise')\n",
      "('proceed', 'proceeded')\n",
      "('rapidli', 'rapidly')\n",
      "('littl', 'little')\n",
      "('outsid', 'outside')\n",
      "('commentari', 'commentary')\n",
      "('dimens', 'dimensions')\n",
      "('implic', 'implications')\n",
      "('hardli', 'hardly')\n",
      "\n",
      " The percentage of valid stems is 51.162790697674424%\n"
     ]
    }
   ],
   "source": [
    "badstem = [3,4,5,7,11,12,16,17,19,20,21,22,23,25,27,30,31,32,35,37,39]\n",
    "\n",
    "print(\"There are {} invalid output stems. \\n\".format(len(badstem)))\n",
    "\n",
    "for idx, word in enumerate(stems):\n",
    "    if idx in badstem:\n",
    "        print(word)        \n",
    "\n",
    "print('\\n The percentage of valid stems is {}%'.format(((len(stems) - len(badstem)) / len(stems)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an RNN with Pre-Trained and Self-Trained Vectors on the IMDB dataset.\n",
    "\n",
    "Do your self-trained vectors show the same behavior as pre-trained (King - man + Woman = Queen)\n",
    "\n",
    "How much do the pretrained vectors change when you allow them to be updated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "import spacy\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Train: 25000\n",
      "Loaded Test: 25000\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size) # num_words: integer or None. Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data.\n",
    "\n",
    "print(\"Loaded Train: {}\".format(len(X_train)))\n",
    "print(\"Loaded Test: {}\".format(len(X_test)))    # 0 is negative sentiment, 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary word to integer index\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift integer + 3 to make way for reserved indices\n",
    "word_index = {k:(v + 3) for k,v in word_index.items()}\n",
    "# insert special token to integers into our dictionary\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# create reverse index mapping to word dict\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad all sequences to be common length, or chop those that extend \n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with Self-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 865,825\n",
      "Trainable params: 865,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "model_self = Sequential()\n",
    "model_self.add(Embedding(vocabulary_size, embedding_size, input_length=max_words)) # <-- layer accepts word numbers, instead of one-hot vectors\n",
    "model_self.add(LSTM(256, unroll=True))\n",
    "model_self.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(model_self.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_self.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint_self = ModelCheckpoint('model_self.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/5\n",
      "24900/25000 [============================>.] - ETA: 2s - loss: 0.5074 - accuracy: 0.7471\n",
      "Epoch 00001: accuracy improved from -inf to 0.74756, saving model to model_self.h5\n",
      "25000/25000 [==============================] - 510s 20ms/sample - loss: 0.5068 - accuracy: 0.7476\n",
      "Epoch 2/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.3792 - accuracy: 0.8368\n",
      "Epoch 00002: accuracy improved from 0.74756 to 0.83636, saving model to model_self.h5\n",
      "25000/25000 [==============================] - 462s 18ms/sample - loss: 0.3797 - accuracy: 0.8364\n",
      "Epoch 3/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.3160 - accuracy: 0.8677\n",
      "Epoch 00003: accuracy improved from 0.83636 to 0.86764, saving model to model_self.h5\n",
      "25000/25000 [==============================] - 493s 20ms/sample - loss: 0.3162 - accuracy: 0.8676\n",
      "Epoch 4/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.3142 - accuracy: 0.8698\n",
      "Epoch 00004: accuracy improved from 0.86764 to 0.86988, saving model to model_self.h5\n",
      "25000/25000 [==============================] - 443s 18ms/sample - loss: 0.3141 - accuracy: 0.8699\n",
      "Epoch 5/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.2645 - accuracy: 0.8953\n",
      "Epoch 00005: accuracy improved from 0.86988 to 0.89540, saving model to model_self.h5\n",
      "25000/25000 [==============================] - 450s 18ms/sample - loss: 0.2644 - accuracy: 0.8954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14327b550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_self.fit(X_train, y_train, batch_size=100, epochs=5, callbacks=[checkpoint_self])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 86.06799840927124\n"
     ]
    }
   ],
   "source": [
    "scores = model_self.evaluate(X_test, t_test, verbose=0)\n",
    "print(\"Test Set Accuracy: {}\".format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with PreTrained Word Embeddings (GloVe Fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GloVe vectors\n",
    "embeddings_index = dict()\n",
    "f = open('WilkeRevEngineering/glove.6B/glove.6B.100d.txt', 'r', encoding=\"UTF-8\")\n",
    "# is in format: the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953.....\n",
    "for line in f:\n",
    "    values = line.split()                              #<-- split each line on spaces\n",
    "    word = values[0]                                   #<-- gets word from first position\n",
    "    coefs = np.asarray(values[1:], dtype='float32')    #<-- gets coefs from every position after 1, convert to np array\n",
    "    embeddings_index[word] = coefs                     #<-- add to dict() word is key, coefs are np array\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))       #<-- initialize empty (0) np array of shape 5000 x 100\n",
    "index = 0                                                 #<-- initialize index\n",
    "for word in word_index:                                   #<-- for each word\n",
    "    index = word_index[word]                              #<-- get index\n",
    "    if index > vocabulary_size - 1:                       #<-- IMDB is sorted by word count frequencies HOW CONVENIENT!\n",
    "        pass                                              #<-- outside of vocabulary size\n",
    "    else:                                                 #<-- inside of vocabulary size\n",
    "        embedding_vector = embeddings_index.get(word)     #<-- retrieve embedding coeffecients by word lookup\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector    #<-- set row of embedding matrix to embedding coeffecients (if exists)\n",
    "embedding_matrix[2] = embeddings_index.get('unk')\n",
    "embedding_matrix[1] = np.ones(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 865,825\n",
      "Trainable params: 365,825\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_glove_fix = Sequential()\n",
    "model_glove_fix.add(Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], input_length=max_words, trainable=False))\n",
    "model_glove_fix.add(LSTM(256, unroll=True))\n",
    "model_glove_fix.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(model_glove_fix.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_fix.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint_glove_fix = ModelCheckpoint('model_glove_fix.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.6209 - accuracy: 0.6612\n",
      "Epoch 00001: accuracy improved from -inf to 0.66136, saving model to model_glove_fix.h5\n",
      "25000/25000 [==============================] - 493s 20ms/sample - loss: 0.6207 - accuracy: 0.6614\n",
      "Epoch 2/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.5290 - accuracy: 0.7385\n",
      "Epoch 00002: accuracy improved from 0.66136 to 0.73856, saving model to model_glove_fix.h5\n",
      "25000/25000 [==============================] - 397s 16ms/sample - loss: 0.5288 - accuracy: 0.7386\n",
      "Epoch 3/5\n",
      "24900/25000 [============================>.] - ETA: 2s - loss: 0.4168 - accuracy: 0.8138\n",
      "Epoch 00003: accuracy improved from 0.73856 to 0.81356, saving model to model_glove_fix.h5\n",
      "25000/25000 [==============================] - 571s 23ms/sample - loss: 0.4169 - accuracy: 0.8136\n",
      "Epoch 4/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.3844 - accuracy: 0.8321\n",
      "Epoch 00004: accuracy improved from 0.81356 to 0.83196, saving model to model_glove_fix.h5\n",
      "25000/25000 [==============================] - 421s 17ms/sample - loss: 0.3846 - accuracy: 0.8320\n",
      "Epoch 5/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.3320 - accuracy: 0.8582\n",
      "Epoch 00005: accuracy improved from 0.83196 to 0.85820, saving model to model_glove_fix.h5\n",
      "25000/25000 [==============================] - 463s 19ms/sample - loss: 0.3319 - accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e75f358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_fix.fit(X_train, y_train, batch_size=100, epochs=5, callbacks=[checkpoint_glove_fix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 86.95600032806396\n"
     ]
    }
   ],
   "source": [
    "scores = model_glove_fix.evaluate(X_test, t_test, verbose=0)\n",
    "print(\"Test Set Accuracy: {}\".format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with PreTrained Word Embeddings (GloVe Fine Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 865,825\n",
      "Trainable params: 865,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_glove_finetune = Sequential()\n",
    "model_glove_finetune.add(Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], input_length=max_words, trainable=True))\n",
    "model_glove_finetune.add(LSTM(256, unroll=True))\n",
    "model_glove_finetune.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(model_glove_finetune.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_finetune.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_glove_finetune = ModelCheckpoint('model_glove_finetune.h5', monitor='accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/5\n",
      "24900/25000 [============================>.] - ETA: 2s - loss: 0.5050 - accuracy: 0.7447\n",
      "Epoch 00001: accuracy improved from -inf to 0.74504, saving model to model_glove_finetune.h5\n",
      "25000/25000 [==============================] - 501s 20ms/sample - loss: 0.5046 - accuracy: 0.7450\n",
      "Epoch 2/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.3877 - accuracy: 0.8303\n",
      "Epoch 00002: accuracy improved from 0.74504 to 0.83028, saving model to model_glove_finetune.h5\n",
      "25000/25000 [==============================] - 443s 18ms/sample - loss: 0.3877 - accuracy: 0.8303\n",
      "Epoch 3/5\n",
      "24900/25000 [============================>.] - ETA: 2s - loss: 0.2922 - accuracy: 0.8820\n",
      "Epoch 00003: accuracy improved from 0.83028 to 0.88192, saving model to model_glove_finetune.h5\n",
      "25000/25000 [==============================] - 619s 25ms/sample - loss: 0.2920 - accuracy: 0.8819\n",
      "Epoch 4/5\n",
      "24900/25000 [============================>.] - ETA: 1s - loss: 0.2290 - accuracy: 0.9103\n",
      "Epoch 00004: accuracy improved from 0.88192 to 0.91020, saving model to model_glove_finetune.h5\n",
      "25000/25000 [==============================] - 466s 19ms/sample - loss: 0.2291 - accuracy: 0.9102\n",
      "Epoch 5/5\n",
      "24900/25000 [============================>.] - ETA: 2s - loss: 0.2030 - accuracy: 0.9212\n",
      "Epoch 00005: accuracy improved from 0.91020 to 0.92120, saving model to model_glove_finetune.h5\n",
      "25000/25000 [==============================] - 502s 20ms/sample - loss: 0.2028 - accuracy: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x156e3c1d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_finetune.fit(X_train, y_train, batch_size=100, epochs=5, callbacks=[checkpoint_glove_finetune])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 87.56399750709534\n"
     ]
    }
   ],
   "source": [
    "scores = model_glove_finetune.evaluate(X_test, t_test, verbose=0)\n",
    "print(\"Test Set Accuracy: {}\".format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_self = model_self.get_weights()[0]\n",
    "embedding_glove_fix = model_glove_fix.get_weights()[0]\n",
    "embedding_glove_finetune = model_glove_finetune.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorSelfTrain(word):\n",
    "    try:\n",
    "        index = word_index[word]\n",
    "        return embedding_self[index]\n",
    "    except KeyError:\n",
    "        print(\"Word not in vocabulary\")\n",
    "\n",
    "def getVectorGloVeFix(word):\n",
    "    try:\n",
    "        index = word_index[word]\n",
    "        return embedding_glove_fix[index]\n",
    "    except KeyError:\n",
    "        print(\"Word not in vocabulary\")\n",
    "\n",
    "def getVectorGloVeTune(word):\n",
    "    try:\n",
    "        index = word_index[word]\n",
    "        return embedding_glove_finetune[index]\n",
    "    except KeyError:\n",
    "        print(\"Word not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Cosine Similarity values for different documents, 1 (same direction), 0 (90 deg.), -1 (opposite directions).\n",
    "# Cosine Distance, 1 - Cosine Similarity, then can take on values from 0 to 2 \n",
    "def cosineDistanceAnalysis(word):\n",
    "    try:\n",
    "        index = word_index[word]\n",
    "        print(\"Distance between Self and GloVe Freeze: {}\".format(distance.cosine(embedding_self[index], embedding_glove_fix[index])))\n",
    "        print(\"Distance between Self and GloVe Tune: {}\".format(distance.cosine(embedding_self[index], embedding_glove_finetune[index])))\n",
    "        print(\"Distance between GloVe Freeze and GloVe Tune: {}\".format(distance.cosine(embedding_glove_fix[index], embedding_glove_finetune[index])))\n",
    "    except KeyError:\n",
    "        print(\"Word not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your self-trained vectors show the same behavior as pre-trained (king - man + woman = queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4907889366149902"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queenselfhat = getVectorSelfTrain(\"king\") - getVectorSelfTrain(\"man\") + getVectorSelfTrain(\"woman\")\n",
    "distance.cosine(queenselfhat, getVectorSelfTrain(\"queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21655863523483276"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queenglovefixhat = getVectorGloVeFix(\"king\") - getVectorGloVeFix(\"man\") + getVectorGloVeFix(\"woman\")\n",
    "distance.cosine(queenglovefixhat, getVectorGloVeFix(\"queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2385583519935608"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queenglovetunehat = getVectorGloVeTune(\"king\") - getVectorGloVeTune(\"man\") + getVectorGloVeTune(\"woman\")\n",
    "distance.cosine(queenglovetunehat, getVectorGloVeTune(\"queen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above exhibits that my self-trained embeddings perform poorly for learning (king - man + woman = queen). The calculated vector and actual vector are quite different. The pre-trained GloVe embeddings show the opposite. They are both able to get pretty close to the actual queen vector when calculating an estimate from (king - man + woman). You'll note that the fine-tuned GloVe embeddings are not as close as the fixed GloVe embeddings, which makes sense empirically if the self-trained embeddings are having trouble learning this relationship, then introducing the IMDB data to fine tune the GloVe embeddings will make them worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.505023717880249"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queenselfhat = getVectorSelfTrain(\"paris\") - getVectorSelfTrain(\"france\") + getVectorSelfTrain(\"britain\")\n",
    "distance.cosine(queenselfhat, getVectorSelfTrain(\"london\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11206841468811035"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queenglovefixhat = getVectorGloVeFix(\"paris\") - getVectorGloVeFix(\"france\") + getVectorGloVeFix(\"britain\")\n",
    "distance.cosine(queenglovefixhat, getVectorGloVeFix(\"london\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1295989751815796"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queenglovetunehat = getVectorGloVeTune(\"paris\") - getVectorGloVeTune(\"france\") + getVectorGloVeTune(\"britain\")\n",
    "distance.cosine(queenglovetunehat, getVectorGloVeTune(\"london\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also holds true for learning (paris - france + britain = london). My self-trained embeddings had a hard time learning this relationship, which sounds right considering they are movie reviews! The GloVe embeddings did very well learning this relationship as the estimated vector is close to the actual vector. The same paradigm holds for the fine-tuned GloVe as above - by allowing a dataset with less data supporting these relationship to update the weights the embeddings lose some of this context. This is why the fine-tuned embeddings become worse than keeping them fixed for this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much do the pretrained vectors change when you allow them to be updated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Self and GloVe Freeze: 1.0866677090525627\n",
      "Distance between Self and GloVe Tune: 1.0833083391189575\n",
      "Distance between GloVe Freeze and GloVe Tune: 0.0014770030975341797\n"
     ]
    }
   ],
   "source": [
    "cosineDistanceAnalysis(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Self and GloVe Freeze: 0.8358397483825684\n",
      "Distance between Self and GloVe Tune: 0.8344792127609253\n",
      "Distance between GloVe Freeze and GloVe Tune: 0.0012821555137634277\n"
     ]
    }
   ],
   "source": [
    "cosineDistanceAnalysis(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Self and GloVe Freeze: 0.9740357659757137\n",
      "Distance between Self and GloVe Tune: 0.9748082738369703\n",
      "Distance between GloVe Freeze and GloVe Tune: 0.004621982574462891\n"
     ]
    }
   ],
   "source": [
    "cosineDistanceAnalysis(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Self and GloVe Freeze: 0.8721013516187668\n",
      "Distance between Self and GloVe Tune: 0.8739651590585709\n",
      "Distance between GloVe Freeze and GloVe Tune: 0.0006364583969116211\n"
     ]
    }
   ],
   "source": [
    "cosineDistanceAnalysis(\"building\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Self and GloVe Freeze: 1.0551324374973774\n",
      "Distance between Self and GloVe Tune: 1.0634262785315514\n",
      "Distance between GloVe Freeze and GloVe Tune: 0.006900012493133545\n"
     ]
    }
   ],
   "source": [
    "cosineDistanceAnalysis(\"professor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually - not much. This is evident in the distance between GloVe Freeze and GloVe Tune vectors for each of the sample words: dog, man, woman, building, and professor above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPTF2",
   "language": "python",
   "name": "nlptf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
