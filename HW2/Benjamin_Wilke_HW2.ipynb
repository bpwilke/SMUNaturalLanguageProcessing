{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benjamin Wilke\n",
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Downloaded from Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pg14640.txt\n",
    "McGuffey's First Eclectic Reader, Revised Edition by William Holmes McGuffey\n",
    "http://www.gutenberg.org/cache/epub/14640/pg14640.txt\n",
    "\n",
    "pg14880.txt\n",
    "McGuffey's Fourth Eclectic Reader by William Holmes McGuffey\n",
    "http://www.gutenberg.org/cache/epub/14880/pg14880.txt\n",
    "\n",
    "pg16751.txt\n",
    "McGuffey's Sixth Eclectic Reader by William Holmes McGuffey\n",
    "http://www.gutenberg.org/cache/epub/16751/pg16751.txt\n",
    "\n",
    "pg19721.txt\n",
    "The Literary World Seventh Reader by Browne, Metcalf, and Withers\n",
    "http://www.gutenberg.org/cache/epub/19721/pg19721.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader and Tokenizer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "def open_Gutenberg(url):\n",
    "    response = request.urlopen(url)\n",
    "    return response.read().decode('utf8')\n",
    "\n",
    "def loadAndPrepReader(file_contents):\n",
    "    file_contents = file_contents.split('\\n')   # split into lines\n",
    "    file_contents = [line.strip('\\r') for line in file_contents]  # remove carriage-returns\n",
    "    start_index = [i for i, s in enumerate(file_contents) if '*** START OF THIS PROJECT' in s] # get start index\n",
    "    end_index = [i for i, s in enumerate(file_contents) if '*** END OF THIS PROJECT' in s] # get end index\n",
    "    return file_contents[start_index[0]+1:end_index[0]-1] # returned sliced file_contents\n",
    "\n",
    "def tokenizer(file_contents):\n",
    "    tokenized_words = [word_tokenize(line) for line in file_contents] # tokenize our raw list of lines\n",
    "    unroll = [item for sublist in tokenized_words for item in sublist] # unroll into single list\n",
    "    return [w for w in unroll if w.isalnum()] # drop special characters, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcGuffey_one = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/14640/pg14640.txt\")))\n",
    "mcGuffey_four = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/14880/pg14880.txt\")))\n",
    "mcGuffey_six = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/16751/pg16751.txt\")))\n",
    "literaryWorld_seven = tokenizer(loadAndPrepReader(open_Gutenberg(\"http://www.gutenberg.org/cache/epub/19721/pg19721.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1 \"Text Difficulty\" Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns percentage of unique words divided by total words, \"lexical diversity\"\n",
    "# I don't like that this lexical_diversity function doesn't account for case, but following the example in the\n",
    "# NLTK book\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def vocab_size(text):\n",
    "    return len(set(word.lower() for word in text)) # this does account for case\n",
    "\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lexical diversity of McGuffey's First Eclectic Reader: 0.18\n",
      "The lexical diversity of McGuffey's Fourth Eclectic Reader: 0.14\n",
      "The lexical diversity of McGuffey's Sixth Eclectic Reader: 0.12\n",
      "The lexical diversity of The Literary World Seventh Reader: 0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"The lexical diversity of McGuffey's First Eclectic Reader: {0:.2f}\".format(lexical_diversity(mcGuffey_one)))\n",
    "print(\"The lexical diversity of McGuffey's Fourth Eclectic Reader: {0:.2f}\".format(lexical_diversity(mcGuffey_four)))\n",
    "print(\"The lexical diversity of McGuffey's Sixth Eclectic Reader: {0:.2f}\".format(lexical_diversity(mcGuffey_six)))\n",
    "print(\"The lexical diversity of The Literary World Seventh Reader: {0:.2f}\".format(lexical_diversity(literaryWorld_seven)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem to align with my expectations. That is - for more basic readers the number of unique words is large(r) compared to the total number of words. This means that the unique words take up more of the total words, which could be a proxy for the level of the writing style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size of McGuffey's First Eclectic Reader: 1141\n",
      "The vocabulary size of McGuffey's Fourth Eclectic Reader: 7579\n",
      "The vocabulary size of McGuffey's Sixth Eclectic Reader: 14038\n",
      "The vocabulary size of The Literary World Seventh Reader: 10220\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary size of McGuffey's First Eclectic Reader: {0}\".format(vocab_size(mcGuffey_one)))\n",
    "print(\"The vocabulary size of McGuffey's Fourth Eclectic Reader: {0}\".format(vocab_size(mcGuffey_four)))\n",
    "print(\"The vocabulary size of McGuffey's Sixth Eclectic Reader: {0}\".format(vocab_size(mcGuffey_six)))\n",
    "print(\"The vocabulary size of The Literary World Seventh Reader: {0}\".format(vocab_size(literaryWorld_seven)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also seems to align with my expectations. The more advanced readers have a higher total vocabulary size. This is clear examining from first, fourth, and sixth. The seventh reader seems to drop back down, but may simply be related to the fact that it's a different series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Vocabulary Size & Lexical Diversity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first issue is that lexical diversity on its own is a poor measure of \"lexical richness\" because usually texts containing a large number of tokens will have lower lexical diversity scores simply based on the fact that the writer must re-use function words (words that have litle lexical meaning). A second issue could be that a text contains a very large vocabulary, but is succint in it's writing, thus producing a high lexical diversity score (but could be quite advanced in terms of the vocabulary). To solve the first issue lexical diversity makes more sense when comparing texts that have similar total token size. The second issue can be analyzed by looking at both lexical diversity and the vocabulary size. Thus I would argue that both measures are required to appropriately measure text difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Normalized Vocabulary Size for Multiple Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalize(x):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    denom = max(x) - min(x) \n",
    "    return list((x - min(x)) / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_texts = [mcGuffey_one, mcGuffey_four, mcGuffey_six, literaryWorld_seven]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1141, 7579, 14038, 10220]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab_size(text) for text in my_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.49918586, 1.0, 0.70396215]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_normal = normalize([vocab_size(text) for text in my_texts])\n",
    "vocab_size_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My normalization utilizes a pretty common approach to feature scaling following: Xnew = (X - Xmin) / (Xmax - Xmin). This will make the largest value equal to 1 and the smallest value equal to 0 with the intermediate values falling between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Long-Word Vocabulary Size & Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_word_vocab_size(text, length):\n",
    "    return len(set(word.lower() for word in text if len(word) > length))\n",
    "\n",
    "def long_word_vocab_words(text, length):\n",
    "    return sorted(set(word.lower() for word in text if len(word) > length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_word_vocab_size(mcGuffey_one, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alternatives',\n",
       " 'combinations',\n",
       " 'contemporary',\n",
       " 'diacriticals',\n",
       " 'explanations',\n",
       " 'illustration',\n",
       " 'illustrations',\n",
       " 'representative',\n",
       " 'representatives',\n",
       " 'vocabularies']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_word_vocab_words(mcGuffey_one, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[112, 2152, 5707, 3877]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[long_word_vocab_size(text, 7) for text in my_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.36461127, 1.0, 0.67292225]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_word_vocab_normal = normalize([long_word_vocab_size(text, 7) for text in my_texts])\n",
    "long_word_vocab_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll consider words longer than 7 characters to be \"long words\" since we're dealing with adolescent level reading material. I re-use my normalization technique from the total vocabulary size here to normalize the long word vocabulary counts across the readers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Combination \"Text Difficulty Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17572269807280513,\n",
       " 0.13828714155338873,\n",
       " 0.11789498246304844,\n",
       " 0.10917464242943518]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_diversity_scores = [lexical_diversity(text) for text in my_texts]  # save lexical diversity to list\n",
    "lexical_diversity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexical_diversity_scores = np.asarray(lexical_diversity_scores)\n",
    "#lexical_diversity_scores = list(1 - lexical_diversity_scores)\n",
    "#lexical_diversity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in HW1, lexical diversity seems problematic as longer and more complicated texts would have a disproportionate score simply based on the number of function words contributing to total length of the text. It seems the length of the text is going to dillute the lexical diversity score in this regard. This was shown in my analysis - texts with low lexical diversity scores seem to be more complex and should have higher lexical diversity scores, but are being misrepresented based on length.\n",
    "\n",
    "I chose not to manipulate the lexical diversity scores from HW1 as I think that's the spirit of HW2.\n",
    "\n",
    "I averaged each of the scores: lexical diversity, normalized vocab, normalize long-word vocab, to produce final \"Text Difficulty Score\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = np.asarray(vocab_size_normal) + np.asarray(long_word_vocab_normal) + np.asarray(lexical_diversity_scores)\n",
    "text_difficulty_score = list(total_scores / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Text Difficulty Score of McGuffey's First Eclectic Reader: 0.06\n",
      "The Text Difficulty Score of McGuffey's Fourth Eclectic Reader: 0.33\n",
      "The Text Difficulty Score of McGuffey's Sixth Eclectic Reader: 0.71\n",
      "The Text Difficulty Score of The Literary World Seventh Reader: 0.50\n"
     ]
    }
   ],
   "source": [
    "print(\"The Text Difficulty Score of McGuffey's First Eclectic Reader: {0:.2f}\".format(text_difficulty_score[0]))\n",
    "print(\"The Text Difficulty Score of McGuffey's Fourth Eclectic Reader: {0:.2f}\".format(text_difficulty_score[1]))\n",
    "print(\"The Text Difficulty Score of McGuffey's Sixth Eclectic Reader: {0:.2f}\".format(text_difficulty_score[2]))\n",
    "print(\"The Text Difficulty Score of The Literary World Seventh Reader: {0:.2f}\".format(text_difficulty_score[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These final Text Difficulty Scores still seem to generally align with our expectations of the complexity of the reader as grade level increases. The normalization generally magnifies the differences between the measured complexity of this set of readers. This score also seems to magnify the differences between McGuffey's Sixth Eclectic Reader and The Literary World Seventh Reader, which by this measure seems to show The Literary World Seventh Reader be significantly easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Simple Neural Net for California Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.fetch_california_housing()\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(data['data'])\n",
    "X.columns = data['feature_names']\n",
    "y = data['target']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 21,201\n",
      "Trainable params: 21,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(100, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "          \n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mean_squared_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example from Dr. Slater's office hours on deriving input dimension:\n",
    "\n",
    "(1 x 8).(8 x 100) + 100 = 900 params\n",
    "\n",
    "(1 x 100).(100 x 100) + 100 = 10100 params\n",
    "\n",
    "(1 x 100).(100 x 100) + 100 = 10100 params\n",
    "\n",
    "(1 x 100).(100 x 1) + 1 = 101 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20640 samples\n",
      "Epoch 1/10\n",
      "20640/20640 [==============================] - 1s 40us/sample - loss: 1.5093 - mean_squared_error: 1.5093\n",
      "Epoch 2/10\n",
      "20640/20640 [==============================] - 1s 42us/sample - loss: 0.8775 - mean_squared_error: 0.8775\n",
      "Epoch 3/10\n",
      "20640/20640 [==============================] - 1s 42us/sample - loss: 0.8416 - mean_squared_error: 0.8416\n",
      "Epoch 4/10\n",
      "20640/20640 [==============================] - 1s 40us/sample - loss: 0.8676 - mean_squared_error: 0.8676\n",
      "Epoch 5/10\n",
      "20640/20640 [==============================] - 1s 44us/sample - loss: 0.7300 - mean_squared_error: 0.7300\n",
      "Epoch 6/10\n",
      "20640/20640 [==============================] - 1s 53us/sample - loss: 0.7522 - mean_squared_error: 0.7522\n",
      "Epoch 7/10\n",
      "20640/20640 [==============================] - 1s 64us/sample - loss: 0.8595 - mean_squared_error: 0.8595\n",
      "Epoch 8/10\n",
      "20640/20640 [==============================] - 1s 54us/sample - loss: 0.8795 - mean_squared_error: 0.8795\n",
      "Epoch 9/10\n",
      "20640/20640 [==============================] - 1s 51us/sample - loss: 0.7420 - mean_squared_error: 0.7420\n",
      "Epoch 10/10\n",
      "20640/20640 [==============================] - 1s 49us/sample - loss: 0.8457 - mean_squared_error: 0.8457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a37ad3128>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X.values, y, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
